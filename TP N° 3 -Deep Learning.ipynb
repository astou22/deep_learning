{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mamadou Korka DIALLO & Mame Astou SENE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Practical Deep Learning Tutorial with PyTorch - Tutorial N° 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020-2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Built ADALINE model using the nn.Module class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Adaline, self).__init__()\n",
    "                                     # une transformation lineare de la forme X.(W.transposé)\n",
    "                                                        #X matrice de donnes qui a (num_features) variables\n",
    "                                                        # chaque donne sera multiplie par un vecteur de poids \n",
    "        self.linear =torch.nn.Linear(num_features, 1)  # de taille (1,num_features), ca donne un veteur de taille\n",
    "                                                        # (num_features,1)\n",
    "        self.linear.weight.detach().zero_()  # changer les poids aleatoire à zero (pour l'initialisation)\n",
    "        self.linear.bias.detach().zero_()  #meme chose pour le bias\n",
    "    def forward(self, x):\n",
    "        activations = self.linear(x)\n",
    "        return activations.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using 'iris.txt', create a binary datasets in 2-D : The last 100 instances of iris described only by the 2nd and 3rd features\n",
    "    \n",
    "    Split the dataset into traing and test sets (70%,30%) \n",
    "\n",
    "    Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('iris.txt', index_col=None, header=None)    #lire le fichier iris\n",
    "df.columns = ['x1', 'x2', 'x3', 'x4', 'y'] #renommer les colonnes\n",
    "df = df.iloc[50:150]   #prendre que la 100 dernière données de la base, donc que les donnees des 2 dernieres classes\n",
    "df['y'] = df['y'].apply(lambda x: 0 if x == 'Iris-versicolor' else 1) #coder la 2e classe par 0 la 3e par 1\n",
    "\n",
    "\n",
    "# Assign features and target\n",
    "\n",
    "X = torch.tensor(df[['x2', 'x3']].values, dtype=torch.float) #ne considerer que la 2e et 3e variables, c'est notre tensor X\n",
    "y = torch.tensor(df['y'].values, dtype=torch.int) #le tensor y correeponds aux labels y\n",
    "\n",
    "# Shuffling & train/test split\n",
    "\n",
    "torch.manual_seed(123)\n",
    "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)  #melanger les indices \n",
    "X, y = X[shuffle_idx], y[shuffle_idx]  #melanger les donnes, ca garde la corrspondance entre une donnée et son label\n",
    "percent70 = int(shuffle_idx.size(0)*0.7)\n",
    "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]  #70 premiers points pour training \n",
    "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]  # 30 dernieres données pour le test\n",
    "\n",
    "# Normalize (mean zero, unit variance)\n",
    "\n",
    "mu, sigma = X_train.mean(dim=0), X_train.std(dim=0) #normalization, soustraire la moyenne diviser par lecarte type\n",
    "X_train =(X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adaline(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Adaline(num_features=X_train.size(1))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model : we will use MSELoss (mean squared error (squared L2 norm)) as loss function. The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, num_epochs, learning_rate, seed):\n",
    "    cost = []\n",
    "    torch.manual_seed(seed)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #use a SGD optimizer\n",
    "    for e in range(num_epochs):\n",
    "        yhat =model.forward(x)   #calcul yhat\n",
    "        loss =loss_func(yhat, y) #calcul the loss function using MSE\n",
    "        optimizer.zero_grad()    # set the gradients to zero\n",
    "        # calculer le gradients\n",
    "        negative_grad_w = grad(loss, model.weight, retain_graph=True)[0] * (-1)\n",
    "        negative_grad_b = grad(loss, model.bias)[0] * (-1)                    \n",
    "        # mise a jour des poids ####\n",
    "        model.weight = model.weight + learning_rate * negative_grad_w\n",
    "        model.bias = model.bias + learning_rate * negative_grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 51.43\n",
      "Test Accuracy: 46.67\n",
      "Weights Parameter containing:\n",
      "tensor([[0., 0.]], requires_grad=True)\n",
      "Bias Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def custom_where(cond, x_1, x_2):\n",
    "    return (cond * x_1) + (torch.logical_not(cond) * x_2)\n",
    "train_pred = model.forward(X_train)\n",
    "train_acc = torch.mean(\n",
    "    (custom_where(train_pred > 0.5, 1, 0).int() == y_train).float())\n",
    "test_pred = model.forward(X_test)\n",
    "test_acc = torch.mean((custom_where(test_pred > 0.5, 1, 0).int() == y_test).float())\n",
    "print('Training Accuracy: %.2f' % (train_acc*100))\n",
    "print('Test Accuracy: %.2f' % (test_acc*100))\n",
    "print('Weights', model.linear.weight)\n",
    "print('Bias', model.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Built a Perceptron model using nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = torch.zeros(num_features, 1, \n",
    "                                   dtype=torch.float32, device=device)\n",
    "        self.bias = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = torch.add(torch.mm(x, self.weights), self.bias)\n",
    "        predictions = custom_where(linear > 0., 1, 0).float()\n",
    "        return predictions\n",
    "\n",
    "        \n",
    "    def backward(self, x, y):  \n",
    "        predictions = self.forward(x)\n",
    "        errors = y - predictions\n",
    "        return errors\n",
    "\n",
    "        \n",
    "    def train(self, x, y, epochs):\n",
    "        for i in range(y.size()[0]):\n",
    "            # use view because backward expects a matrix (i.e., 2D tensor)\n",
    "            errors = self.backward(x[i].view(1, self.num_features), y[i]).view(-1)\n",
    "            self.weights += (errors * x[i]).view(self.num_features, 1)\n",
    "            self.bias += errors\n",
    "    def evaluate(self, x, y):\n",
    "        predictions = self.forward(x).view(-1)\n",
    "        accuracy = torch.sum(predictions == y).float() / y.size()[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the 'perceptron_toydata' dataset\n",
    "\n",
    "    Split the dataset into train and test sets\n",
    "    \n",
    "    Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label counts: tensor([50, 50])\n",
      "X.shape: torch.Size([100, 2])\n",
      "y.shape: torch.Size([100])\n",
      "torch.Size([70, 2])\n",
      "torch.Size([70])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('perceptron_toydata.txt', index_col=None, header=None , delimiter='\\t')\n",
    "df.columns = ['x1', 'x2', 'y']\n",
    "X = torch.tensor(df[['x1', 'x2']].values, dtype=torch.float) \n",
    "y = torch.tensor(df['y'].values, dtype=torch.int) \n",
    "print('Class label counts:', torch.bincount(y))\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)\n",
    "\n",
    "# Shuffling & train/test split\n",
    "shuffle_idx = torch.randperm(y.size(0), dtype=torch.long)\n",
    "X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "percent70 = int(shuffle_idx.size(0)*0.7)\n",
    "X_train, X_test = X[shuffle_idx[:percent70]], X[shuffle_idx[percent70:]]\n",
    "y_train, y_test = y[shuffle_idx[:percent70]], y[shuffle_idx[percent70:]]\n",
    "# Normalize (mean zero, unit variance)\n",
    "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test = (X_test - mu) / sigma\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "  Weights: tensor([[1.0686],\n",
      "        [1.2897]])\n",
      "  Bias: tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = Perceptron(num_features=2)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "model.train(X_train_tensor, y_train_tensor, epochs=5)\n",
    "\n",
    "print('Model parameters:')\n",
    "print('  Weights: %s' % model.weights)\n",
    "print('  Bias: %s' % model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. evaluate the model (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device)\n",
    "\n",
    "test_acc = model.evaluate(X_test_tensor, y_test_tensor)\n",
    "print('Test set accuracy: %.2f%%' % (test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the single-layer perceptron, the Multi Layer Perceptron models have hidden layers\n",
    "between the input and the output layers. After every hidden layer, an activation function \n",
    "is applied to introduce non-linearity. \n",
    "\n",
    "9. Built a simple Multi Layer Perceptron model withe one hidden layer. \n",
    "After the hidden layer, we will use ReLU as activation before the information is sent to the output layer.\n",
    "As an output activation function, we will use Sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, num_features,hidden_size):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.num_features, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a random datasets and assign binary labels {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 2])\n",
      "torch.Size([40])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "def blob_label(y, label, loc): # assign labels\n",
    "    target = np.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "    return target\n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define the model with input dimension 2 and hidden dimension 10. \n",
    "Since the task is to classify binary labels, we can use as criterion BCELoss (Binary Cross Entropy Loss) : loss function.\n",
    "The optimizer is SGD (Stochastic Gradient Descent) with learning rate 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(2,10)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer  = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Check the test loss before the model training and compare it with the test loss after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.7927604913711548\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7370143532752991\n",
      "Epoch 1: train loss: 0.7052677869796753\n",
      "Epoch 2: train loss: 0.6764755249023438\n",
      "Epoch 3: train loss: 0.6501957774162292\n",
      "Epoch 4: train loss: 0.6260536313056946\n",
      "Epoch 5: train loss: 0.6038469672203064\n",
      "Epoch 6: train loss: 0.5833786725997925\n",
      "Epoch 7: train loss: 0.5644609332084656\n",
      "Epoch 8: train loss: 0.5469381809234619\n",
      "Epoch 9: train loss: 0.5306822061538696\n",
      "Epoch 10: train loss: 0.5155693292617798\n",
      "Epoch 11: train loss: 0.5014898180961609\n",
      "Epoch 12: train loss: 0.4883461594581604\n",
      "Epoch 13: train loss: 0.4760517179965973\n",
      "Epoch 14: train loss: 0.4645295739173889\n",
      "Epoch 15: train loss: 0.45371121168136597\n",
      "Epoch 16: train loss: 0.4435352683067322\n",
      "Epoch 17: train loss: 0.4339474141597748\n",
      "Epoch 18: train loss: 0.4248986840248108\n",
      "Epoch 19: train loss: 0.41634511947631836\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred =  model(x_train)\n",
    "    y_pred=y_pred.to(torch.device('cpu'))\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.889090359210968\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. In order to improve the model, you can try out different parameter values for your\n",
    "hyperparameters(ie. hidden dimension size, epoch size, learning rates). You can also \n",
    "try changing the structure of your model (ie. adding more hidden layers) to see if your\n",
    "mode improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, num_features,hidden_size):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.num_features, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        relu = self.fc2(relu)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc3(relu)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(2,20)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer  = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.48171600699424744\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.661676824092865\n",
      "Epoch 1: train loss: 0.6060032844543457\n",
      "Epoch 2: train loss: 0.5588164925575256\n",
      "Epoch 3: train loss: 0.5185997486114502\n",
      "Epoch 4: train loss: 0.4840877056121826\n",
      "Epoch 5: train loss: 0.45425185561180115\n",
      "Epoch 6: train loss: 0.4282664656639099\n",
      "Epoch 7: train loss: 0.40547090768814087\n",
      "Epoch 8: train loss: 0.38533473014831543\n",
      "Epoch 9: train loss: 0.3674440085887909\n",
      "Epoch 10: train loss: 0.3514552414417267\n",
      "Epoch 11: train loss: 0.3370722830295563\n",
      "Epoch 12: train loss: 0.3240661323070526\n",
      "Epoch 13: train loss: 0.31225156784057617\n",
      "Epoch 14: train loss: 0.30149155855178833\n",
      "Epoch 15: train loss: 0.29163801670074463\n",
      "Epoch 16: train loss: 0.2825690507888794\n",
      "Epoch 17: train loss: 0.2741919159889221\n",
      "Epoch 18: train loss: 0.2664278447628021\n",
      "Epoch 19: train loss: 0.2592092454433441\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred =  model(x_train)\n",
    "    y_pred=y_pred.to(torch.device('cpu'))\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.7431154251098633\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
